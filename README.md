# OSWorld-v2 Task Collection & Implementation Guide

## 0. Environment Setup

### General Setup with AWS
The runtime environment is built on the **OSWorld framework**, primarily operating within **AWS evaluation environments**. Complete setup instructions can be found in [PUBLIC_EVALUATION_GUIDELINE](https://github.com/yuanmengqi/OSWorld-V2/blob/main/PUBLIC_EVALUATION_GUIDELINE.md).

To simplify the setup process, we have preconfigured security groups, the host AMI, and other components. Below is a simple method to set up an AWS host. 

The configuration parameters are as follows:

| Configuration Item | Value                                                        |
| ------------------ | ------------------------------------------------------------ |
| AMI ID             | `ami-0cfb5f11de0f27c5c`                                      |
| Instance Type      | - `t3.medium` (Recommended for ≤5 parallel tasks)<br />- `t3.large` (Recommended for ≤15 parallel tasks)<br /><br /> - These numbers are based on using VSCode over SSH. You can save resources by running via CLI—`t3.large` supports up to 20 tasks that way.<br /> - For higher parallelism, use a more powerful instance.<br /> - **For development only, we recommend using t3.large** |
| VPC                | `vpc-0f207282fe145bcda`                                      |
| Subnet             | `subnet-0a4b0c5b8f6066712`                                   |
| Security groups    | `sg-05f8e79c10a7768e4`                                       |
| Storage            | We recommend selecting 50GB, which can be expanded later if needed |

Reference for configuration parameter locations on AWS website:
<p>
  <img src="https://github.com/yuanmengqi/OSWorld-V2/blob/main/assets/aws-1.png" width="80%" />
  <img src="https://github.com/yuanmengqi/OSWorld-V2/blob/main/assets/aws-2.png" width="80%" />
</p>
(Here you can create a key to obtain your instance's key file)

After launching, you can see the public DNS by opening the host instance:
![aws-3](https://github.com/yuanmengqi/OSWorld-V2/blob/main/assets/aws-3.png)

At this point, you have the instance key and DNS. Here's what you need to do:
1. Save the provided `osworld-host-key-xxx.pem` key to your local machine and execute `chmod 400 "osworld-host-key-xxx.pem"` to grant permissions to the key.
2. Connect to the host instance using the provided information. You have two ways to connect:
   1. SSH:
     ```
     ssh -i <your_key_path> ubuntu@<your_public_dns>
     ```
   2. VSCode:
     ```
     Host host_example
       HostName <your_public_dns>
       User ubuntu
       IdentityFile <your_key_path>
     ```
3. The code stored in the AWS image may not be the latest version. To avoid issues caused by version differences, please first log into your GitHub account and pull the latest code from the osworld-v2 repository.

**Note**: If you are using a lab-provided instance, you can directly use the conda environment `osworld` without additional configuration. We will notify everyone if environment configuration updates are needed in the future.
**Note**: The `/home/ubuntu/OSWorld-V2/.env_vars` file in the instance contains the environment variables required to run the code. Please complete it with the API parameters provided by the lab and set these environment variables before running the code.


## 1. Task Selection Criteria

We are looking for **real, complex computer-use tasks**.  Please feel free to propose tasks that:

- Take **more than 50 steps or 10 minutes** to complete, or  
- You have attempted with existing agents and found to be **inaccessible for the agent** to complete.

These tasks can come from your **daily life, study, or work**.  Whenever you find yourself thinking, *“This task takes too much time, I wish an agent could handle it,”* it may be a valuable task to consider.  

If the task also has **practical or business value**, that would be especially appreciated.

When designing your task, please keep the following points in mind:

1. **Avoid Ambiguity:**  Please ensure clear and specific instructions to avoid any confusion. 

   For example, instead of:  “Schedule a meeting with Amy at 3 PM on Jan 1, 2025,”  you need to specify **where** the meeting should occur e.g., “Schedule the meeting on Google Calendar” or “Create a Zoom meeting.”

2. **Ensure a Unique Result:**  The result should be **objectively verifiable**.  

   For example, a task like “Find a route from A to B” may be unsuitable because there can be many valid paths.

3. **Avoid Time-Sensitive Tasks:**  

   Tasks that rely on rapidly changing data (e.g., “Find the top 10 AI universities in the USA on CSRankings”) may be unstable for evaluation purposes.

4. **Evaluation Method:**  

   We recommend **function-based evaluation** where possible. However, **model-based evaluation** is also supported if it is stable and reproducible.  

   For example, model evaluation is appropriate for verifying *“whether the inserted text is centered in the image”*, but may not be suitable for more subjective judgments such as *“how good is this summary.”*

---
## 2. Task Implementation Workflow

A task in OSWorld typically involves four stages to be completed:

1. **Environment Setup** – Preparing the initial Ubuntu environment for the agent.  
2. **Agent Execution** – The agent performs the required actions to complete the task.  
3. **Post-Processing** – Save system states or results after execution.  
4. **Evaluation** – Compare the results with the ground truth using function or model evaluation.

### File Preparation

All task-related files should be stored in the following shared Drive:   [Google Drive Folder](https://drive.google.com/drive/folders/1N4f5mTmYtVLC2uEATKk6eD_ZTKoY96gN?usp=sharing)

Each task file should use task_xxx as the file prefix. Each task requires uploading files that include:

- **Initial files** used during the setup stage  
- **Ground truth files** used in the evaluation stage

### Code Implementation

The OSWorld-task-collection repository is located at:  [GitHub – OSWorld-task-collection](https://github.com/yuanmengqi/OSWorld-task-collection/tree/main)

You will need to implement two main components:

1. **Task Configuration File**  stored in:  `evaluation_examples/examples`
    The configuration file should include:

    - The task ID and instruction
    - The setup steps
    - Post-processing and evaluation settings

2. **Evaluation Functions**  

   Please implement or reference the necessary functions in the following directories:`OSWorld-task-collection/desktop_env/evaluators/getters` and `OSWorld-task-collection/desktop_env/evaluators/metrics`

---

## 3. Example Task

### Example 1

#### Step 1: Write the Task Instruction

> “The file 'transcript_Hua_Li.pdf' contains the courses I have already completed, while 'Computer_Science_and_Technology_Program_Curriculum_Class_of_2025.pdf' outlines my program requirements. Please calculate the remaining credits for each module in the 'Compulsory Courses' and 'Elective Courses' sections. For the 'Core General Education Courses' module, the remaining credits should be calculated as 160 total credits minus the credits I have already earned and the remaining credits for other modules. Fill in the 'Remaining_Course.xlsx' file accordingly. Additionally, list the names of any compulsory courses I have not taken in the empty spaces under 'Remaining Courses.'”

#### Step 2: Prepare Files

- Prepare the initial files needed for setup: `Computer_Science_and_Technology_Program_Curriculum_Class_of_2025.pdf`, `transcripts_Hua_Li.pdf`, and `Remaining_Course.xlsx`.
- Prepare the ground truth file needed for evaluation: `Remaining_Course_gt.xlsx`.
- Upload all prepared files to Google Drive.

#### Step 3: Create the Task Config

Edit the task config file located at: `OSWorld-task-collection/evaluation_examples/new_tasks/example.json`

Some key parameters include:

- **"id"**: Assign a unique ID to the task and store this ID in the `new_tasks` section of the file `OSWorld-task-collection/evaluation_examples/test_new.json`.

- **"instruction"**: Enter the instruction provided in step 1.

- **"config"**: This parameter controls the setup steps. For this task, it involves downloading the initial files and opening them in LibreOffice. The code is as follows:

  ```
  "config": [
        {
          "type": "download",
          "parameters": {
            "files": [
              {
                "url": "https://drive.google.com/uc?id=1nN2RUs25QmD8EnmwOV3vqBxh7o_QQ2-j&export=download",
                "path": "/home/user/Desktop/Computer_Science_and_Technology_Program_Curriculum_Class_of_2025.pdf"
              },
              {
                "url": "https://drive.google.com/uc?id=1GWKJdtGUpng4eOxQ_HL1spHOpB--tAT_&export=download",
                "path": "/home/user/Desktop/transcripts_Hua_Li.pdf"
              },
              {
                "url": "https://drive.google.com/uc?id=16KgU8wQGgFFLEr_7IPdvY6UYecog0KoF&export=download",
                "path": "/home/user/Desktop/Remaining_Course.xlsx"
              }
            ]
          }
        },
        {
          "type": "open",
          "parameters": {
            "path": "/home/user/Desktop/Remaining_Course.xlsx"
          }
        }
      ]
  ```

- **"evaluator"**: This parameter contains the evaluation steps.

  - **"postconfig"**: This parameter includes post-processing steps. In this example, it saves the edited file. The code is as follows:

    ```
    "postconfig": [
            {
              "type": "activate_window",
              "parameters": {
                "window_name": "Remaining_Course.xlsx - LibreOffice Calc",
                "strict": true
              }
            },
            {
              "type": "sleep",
              "parameters": {
                "seconds": 0.5
              }
            },
            {
              "type": "execute",
              "parameters": {
                "command": [
                  "python",
                  "-c",
                  "import pyautogui; import time; pyautogui.hotkey('ctrl', 's'); time.sleep(0.5);"
                ]
              }
            },
            {
              "type": "sleep",
              "parameters": {
                "seconds": 3
              }
            }
          ]
    ```

  - **"func"**: This calls a function from the metrics to select the appropriate evaluation method. In this task, it uses the Excel comparison function:

    ```
    "func": "compare_table"
    ```

    - **"expected"**: This loads the ground truth file using the `getter` function. In this task, the ground truth file is downloaded:

    ```
    "expected": {
            "type": "cloud_file",
            "path": "https://drive.google.com/uc?id=1aEd-3q_78Eo5u6HPAJcd9JGP8i2NOoX9&export=download",
            "dest": "Remaining_Course_gt.xlsx"
          }
    ```

    - **"result"**: This retrieves the result of the agent's actions using the `getter` function. In this task, it loads the edited file:

    ```
    "result": {
            "type": "vm_file",
            "path": "/home/user/Desktop/Remaining_Course.xlsx",
            "dest": "Remaining_Course.xlsx"
          }
    ```

    - **"options"**: This sets parameters for the evaluation function. In this task, it specifies which cells in the Excel file to compare and how. This section depends on the specific definition of the functions in the metrics.

    ```
    "options": {
        "rules": [
            {
              "type": "sheet_fuzzy",
              "sheet_idx0": "RI0",
              "sheet_idx1": "EI0",
              "rules": [
                {
                  "range": [
                    "B3:E3"
                  ],
                  "type": "exact_match",
                  "trim_leadings": " ",
                  "trim_trailings": " "
                },
                {
                  "range": [
                    "C8:E8"
                  ],
                  "type": "exact_match",
                  "trim_leadings": " ",
                  "trim_trailings": " ",
                  "ignore_case": true,
                  "allow_empty_when_expected_none": true
                },
                {
                  "range": [
                    "B7"
                  ],
                  "type": "exact_match",
                  "trim_leadings": " ",
                  "trim_trailings": " "
                }
              ]
            }
          ]
      }
    ```

#### **Step 4: Edit the necessary functions** 

You need to edit the functions in `OSWorld-task-collection/desktop_env/evaluators/getters` and `OSWorld-task-collection/desktop_env/evaluators/metrics` based on the steps mentioned above.

 (This directory already contains many functions that can be reused, and you may add new ones if necessary.)

- **Getters**: These functions are used to get the ground truth and results after the agent's actions. In this task, we use the `get_cloud_file` function to download a file from the cloud server, and the `get_vm_file` function to get a file from the VM.

  ```
  def get_cloud_file(env, config: Dict[str, Any]) -> Union[str, List[str]]:
      if not config.get("multi", False):
          paths: List[str] = [config["path"]]
          dests: List[str] = [config["dest"]]
      else:
          paths: List[str] = config["path"]
          dests: List[str] = config["dest"]
      cache_paths: List[str] = []
  
      gives: Set[int] = set(config.get("gives", [0]))
  
      for i, (p, d) in enumerate(zip(paths, dests)):
          _path = os.path.join(env.cache_dir, d)
          if i in gives:
              cache_paths.append(_path)
  
          if os.path.exists(_path):
              continue
  
          url = p
          response = requests.get(url, stream=True)
          response.raise_for_status()
  
          with open(_path, 'wb') as f:
              for chunk in response.iter_content(chunk_size=8192):
                  if chunk:
                      f.write(chunk)
  
      return cache_paths[0] if len(cache_paths)==1 else cache_paths
  ```

- **Metrics**: These functions are used to compare the ground truth and the agent's results. In this task, we use the `compare_table` function to compare Excel files:

  ```
  def compare_table(result: str, expected: str = None, **options) -> float:
      if result is None:
          logger.error("Result file path is None")
          return 0.0
  
      if not os.path.exists(result):
          logger.error(f"Result file not found: {result}")
          return 0.0
  
      try:
          logger.info(f"Loading result file: {result}")
          xlworkbookr: Workbook = openpyxl.load_workbook(filename=result)
          pdworkbookr = pd.ExcelFile(result)
          logger.info(
              f"Successfully loaded result file with sheets: {pdworkbookr.sheet_names}"
          )
      except Exception as e:
          logger.error(f"Failed to load result file {result}: {e}")
          return 0.0
      worksheetr_names: List[str] = pdworkbookr.sheet_names
  
      if expected is not None:
          xlworkbooke: Workbook = openpyxl.load_workbook(filename=expected)
          pdworkbooke = pd.ExcelFile(expected)
          worksheete_names: List[str] = pdworkbooke.sheet_names
      else:
          xlworkbooke: Workbook = None
          pdworkbooke = None
          worksheete_names: List[str] = None
  
      parse_idx: Callable[[Union[str, int], BOOK, BOOK], Tuple[BOOK, str]] = (
          functools.partial(
              _parse_sheet_idx,
              result_sheet_names=worksheetr_names,
              expected_sheet_names=worksheete_names,
          )
      )
  
      passes = True
      for r in options["rules"]:
          if r["type"] == "sheet_name":
              metric: bool = worksheetr_names == worksheete_names
              logger.debug(
                  "Assertion: %s.sheet_names == %s.sheet_names - %s",
                  result,
                  expected,
                  metric,
              )
  
          elif r["type"] == "sheet_data":
  ......
  ```

* If you define any **new functions**, please remember to import them in `__init__.py`.

#### Step 5: Verify the task implementation

We provide a script that allows you to manually execute the task on the AWS platform, replacing the agent’s behavior, to easily test whether the setup and evaluation functions are correct. The script is located at `OSWorld-task-collection/manual_examine.py`.

1. **Run the script**:

   ```
   python manual_examine.py \
       --headless \
       --observation_type screenshot \
       --result_dir ./results_new_tasks \         # Path to store evaluation results
       --test_all_meta_path evaluation_examples/test_new.json \  # JSON file containing the new task ID
       --region us-east-1 \                       # Set to your AWS region
       --domain new_tasks \                       # Folder containing the task
       --example_id 'example' \                       # Task ID
       --max_steps 3
   ```

2. **Open the VNC link** displayed in the terminal and log in to the visualization VM using the password `osworld-public-evaluation`.

   ![image-20251017173538562](https://github.com/yuanmengqi/OSWorld-task-collection/blob/main/assets/image-20251017173538562.png)

   ![image-20251017173822126](https://github.com/yuanmengqi/OSWorld-task-collection/blob/main/assets/image-20251017173822126.png)

3. **Manually complete the task** ：
    Manually complete the task in the VNC environment and follow the instructions in the terminal to press the enter key to begin the evaluation. Check the evaluation results to ensure the setup and evaluation functions are correct.
    * Please check if the evaluation has reward attack issues, i.e., whether there are cases where no operations, partial operations, or incorrect operations are performed but still incorrectly receive a score of 1.
    * Please check if there are multiple paths to complete the task. If they exist, ensure that the evaluation code can identify all successful examples.
    * If there are model evaluation tasks, please test whether the model evaluation results are stable and whether minor changes will interfere with the evaluation results.
    * If you modify files when editing tasks, please delete the contents under the `OSWorld-v2/cache` path!

#### Step 6: Test task instructions and implementation with agents
Connect agents (here we require using both operator and Claude for evaluation) to run individual tasks and test whether the task instructions and evaluation are correct. Since agents may provide different solutions each time, it is recommended to test 3-5 times.
* Please check if the instructions have ambiguity that could mislead the agent.
* Please check if the agent completes the task in ways that were not previously considered. If so, additional evaluation methods need to be added.
* If you modify files when editing tasks, please delete the contents under the `OSWorld-v2/cache` path!

For operator, we recommend using the following script:
```
python run_multienv_openaicua.py \
--headless \
--observation_type screenshot \
--model computer-use-preview \
--result_dir ./results_new_examples_v2_operator \
--test_all_meta_path evaluation_examples/test_v2.json \
--region us-east-1 \
--max_steps 100 \
--num_envs 1 \
--client_password osworld-public-evaluation
```
For Claude agent, we recommend using the following script:
```
python run_multienv_claude.py \
--headless \
--observation_type screenshot \
--action_space claude_computer_use \
--model claude-4-sonnet-20250514 \
--result_dir ./results_new_examples_v2_claude \
--test_all_meta_path evaluation_examples/test_v2.json \
--max_steps 100 \
--num_envs 1 \
--provider_name aws \
--client_password osworld-public-evaluation
```

### Example 2 (self-host website)
#### Overview
For tasks requiring interaction with websites that have dynamic structures, security measures, or data access limitations, we use self-hosted replicas. This example demonstrates how to create a task using a self-hosted website (Task 080 - International Student Insurance).
#### Step 1: Prepare Your Website
Your website submission should follow this structure:
```
your-website-name/
├── README.md           # Documentation
├── start.sh            # Startup script (optional, for complex setups)
├── index.html          # Main page
├── css/
├── js/
└── ...other files
```
**Recommended Practices:**
We strongly encourage using pure HTML, CSS, and JavaScript for website replicas whenever possible. This approach offers several advantages:
- Faster deployment and startup time
- No build process or dependency installation required
- Easier debugging and maintenance
- Lower resource consumption
Only consider using frameworks (React, Vue, Angular, etc.) when the original website's interactivity or complexity cannot be reasonably replicated with vanilla JavaScript.
#### Step 2: Website Startup
**Option A: Direct Command (Recommended for Simple Cases)**
For simple static websites, execute the startup command directly in the task configuration:
```json
{
    "type": "execute",
    "parameters": {
        "command": ["bash", "-c", "cd /home/user/website/your-website-name && python3 -m http.server 8000 > /tmp/website.log 2>&1 &"]
    }
}
```

**Option B: Using start.sh Script (For Complex Setups)**

If your website requires multiple setup steps, dependency installation, or complex startup logic, create a `start.sh` script:

```bash
#!/bin/bash
# Get the directory where this script is located
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$SCRIPT_DIR"

# Start Python HTTP server in background
python3 -m http.server 8000 &

echo "Website started on http://localhost:8000"
```

**Key Requirements:**
- Commands must run in background (using `&`)
- Use relative paths or environment variables (no hardcoded absolute paths)
- Document the port number in your README

#### Step 3: Package Your Website

Create a zip file of your website:

```bash
cd your-website-name
zip -r ../your-website-name.zip .
```

Place the zip file in the appropriate directory:
```
OSWorld-V2/
└── self_hosted_websites/
    └── your-website-name/
        └── your-website-name.zip
```

#### Step 4: Initiate the Task Configuration with your Websites

**Example Using Direct Command (Recommended for Static Sites):**

```json
{
    "config": [
        {
            "type": "upload_file",
            "parameters": {
                "files": [{
                    "local_path": "./self_hosted_websites/your-website-name/your-website-name.zip",
                    "path": "/home/user/website.zip"
                }]
            }
        },
        {
            "type": "execute",
            "parameters": {
                "command": ["unzip", "/home/user/website.zip", "-d", "/home/user/website"]
            }
        },
        {
            "type": "execute",
            "parameters": {
                "command": ["bash", "-c", "cd /home/user/website/your-website-name && python3 -m http.server 8000 > /tmp/website.log 2>&1 &"]
            }
        },
        {
            "type": "sleep",
            "parameters": {"seconds": 2}
        },
        {
            "type": "launch",
            "parameters": {
                "command": ["firefox", "http://localhost:8000"]
            }
        }
    ]
}
```

**Example Using start.sh Script (For Complex Setups):**

```json
{
    "config": [
        ...
        {
            "type": "execute",
            "parameters": {
                "command": ["bash", "-c", "bash /home/user/website/your-website-name/start.sh > /tmp/website-start.log 2>&1 &"]
            }
        },
        ...
    ]
}
```

Note: Always include `&` at the end of commands to run them in background and prevent timeout errors.

#### Step 5: Test Your Submission

**Local Test:**
```bash
cd your-website-name
chmod +x start.sh  # if using start.sh
./start.sh         # or run your command directly
# Wait a moment, then open http://localhost:8000 in your browser
```

**Integration Test:**
```bash
python manual_examine.py \
    --headless \
    --observation_type screenshot \
    --result_dir ./results_test \
    --test_all_meta_path evaluation_examples/test_v2.json \
    --region us-east-1 \
    --domain tasks \
    --example_id '080' \
    --max_steps 20
```

---

### Example 3 (model eval)

#### Overview

Use LLM-based evaluation when results require **subjective judgment** that's difficult to verify programmatically, such as visual quality, content relevance, or format correctness. The LLM acts as a judge, comparing outputs against references and returning binary pass/fail results.

**Critical Requirement**: Evaluation criteria must have **clear, unambiguous answers**. Avoid subjective judgments like "Is this summary good?" Instead, use verifiable checks like "Does the summary contain all key points from the reference?"

#### Example Configuration (Task 003 - Image Editing)

```json
"evaluator": {
    "func": ["compare_images_with_llm", "compare_images_with_llm", "compare_images_with_llm"],
    "conj": "and",
    "result": [
        {"type": "vm_file", "path": "/home/user/Desktop/Hongkong_raindrop.jpeg", "dest": "Hongkong_result.jpeg"}
    ],
    "expected": [
        {"type": "cloud_file", "path": "https://raw.githubusercontent.com/.../Hongkong.jpg", "dest": "Hongkong_bg.jpeg"}
    ],
    "options": [
        {"prompt": "The first image shows a night view of Hong Kong. The second image should feature the same cityscape with a raindrop foreground overlay and a 'Hong Kong' text label. Check if: (1) the cityscapes are different, (2) raindrops are missing, or (3) the text label is missing."}
    ]
}
```

#### Configuration Fields

- **`func`**: Evaluation function name(s) - array for multiple checks, string for single check
  - Available: `compare_images_with_llm`, `compare_files_with_llm`, or custom functions
- **`conj`**: Logical operator for multiple evaluations
  - `"and"`: All must pass (default)
  - `"or"`: Any one passing is sufficient
- **`result`**: Agent's output files (fetched after task execution)
- **`expected`**: Reference files (ground truth)
- **`options`**: Evaluation prompts (must match `func` array length)

#### Writing Effective Prompts

**Required Structure:**
1. Describe what the reference contains
2. State explicit, verifiable requirements for the result
3. List specific failure conditions to check

**Good Example (Clear & Verifiable):**
```
"The reference file contains 5 cities. The result must list the same 5 cities in the same order. Check if: (1) cities are missing, (2) cities are in different order, or (3) extra cities are added."
```

**Bad Example (Ambiguous):**
```
"Check if the result looks similar to the reference and seems correct."
```

**Best Practices:**
- Use objective, binary criteria ("contains X", "matches format Y")
- Provide explicit negative checks ("missing", "incorrect", "extra")
- Avoid subjective terms ("good", "beautiful", "appropriate")
- Test with edge cases to ensure consistent YES/NO responses

#### Implementation

The evaluation function (e.g., `compare_images_with_llm`):
1. Loads both result and reference files
2. Encodes content (base64 for images, text for documents)
3. Sends to GPT-4 Vision/GPT-4 with evaluation prompt
4. Parses response: "YES" → 1.0, "NO" → 0.0

**Environment Setup:**
```bash
export OPENAI_API_KEY_CUA="your-api-key"
```

**API Configuration:**
- Model: `gpt-4.1` (supports vision and text)
- Max tokens: 10 (binary response)
- Temperature: 0 (deterministic)

#### When to Use LLM Evaluation

**Appropriate:**
- Visual verification (element presence, positioning, layout)
- Content matching (text contains required information)
- Format validation (structure follows template)
- Multi-modal checks (combining image + text requirements)

**Not Appropriate:**
- Purely subjective judgments without clear criteria
- Tasks with reliable programmatic solutions (exact string match, file size, etc.)
- Evaluation requiring mathematical precision

#### Testing Your LLM Evaluator

1. **Determinism Test**: Run 5+ times with same inputs → should always get same result
2. **Positive Cases**: Verify correct outputs receive 1.0
3. **Negative Cases**: Verify various types of failures receive 0.0
4. **Edge Cases**: Test boundary conditions (partial completion, alternative valid solutions)

If results are inconsistent, revise your prompt to be more explicit and objective.

---