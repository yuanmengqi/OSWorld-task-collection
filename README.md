# OSWorld-v2 Task Collection & Implementation Guide

## 1. Task Selection Criteria

We are looking for **real, complex computer-use tasks**.  Please feel free to propose tasks that:

- Take **more than 50 steps or 10 minutes** to complete, or  
- You have attempted with existing agents and found to be **inaccessible for the agent** to complete.

These tasks can come from your **daily life, study, or work**.  Whenever you find yourself thinking, *“This task takes too much time, I wish an agent could handle it,”* it may be a valuable task to consider.  

If the task also has **practical or business value**, that would be especially appreciated.

When designing your task, please keep the following points in mind:

1. **Avoid Ambiguity:**  Please ensure clear and specific instructions to avoid any confusion. 

   For example, instead of:  “Schedule a meeting with Amy at 3 PM on Jan 1, 2025,”  you need to specify **where** the meeting should occur e.g., “Schedule the meeting on Google Calendar” or “Create a Zoom meeting.”

2. **Ensure a Unique Result:**  The result should be **objectively verifiable**.  

   For example, a task like “Find a route from A to B” may be unsuitable because there can be many valid paths.

3. **Avoid Time-Sensitive Tasks:**  

   Tasks that rely on rapidly changing data (e.g., “Find the top 10 AI universities in the USA on CSRankings”) may be unstable for evaluation purposes.

4. **Evaluation Method:**  

   We recommend **function-based evaluation** where possible. However, **model-based evaluation** is also supported if it is stable and reproducible.  

   For example, model evaluation is appropriate for verifying *“whether the inserted text is centered in the image”*, but may not be suitable for more subjective judgments such as *“how good is this summary.”*

---

## 2. Environment Setup

The runtime environment is built on the **OSWorld framework**, primarily operating within **AWS evaluation environments**.  For detailed configuration steps, please refer to: [PUBLIC_EVALUATION_GUIDELINE](https://github.com/yuanmengqi/OSWorld-task-collection/blob/main/PUBLIC_EVALUATION_GUIDELINE.md)

We also support **Docker** and **VMware** environments.  Further setup details can be found in [OSWorld_readme](https://github.com/yuanmengqi/OSWorld-task-collection/blob/main/README_OSWorld.md)

---

## 3. Task Implementation Workflow

A task in OSWorld typically involves four stages to be completed:

1. **Environment Setup** – Preparing the initial Ubuntu environment for the agent.  
2. **Agent Execution** – The agent performs the required actions to complete the task.  
3. **Post-Processing** – Save system states or results after execution.  
4. **Evaluation** – Compare the results with the ground truth using function or model evaluation.

### File Preparation

All task-related files should be stored in the following shared Drive:   [Google Drive Folder](https://drive.google.com/drive/folders/1N4f5mTmYtVLC2uEATKk6eD_ZTKoY96gN?usp=sharing)

Each task file should use task_xxx as the file prefix. Each task requires uploading files that include:

- **Initial files** used during the setup stage  
- **Ground truth files** used in the evaluation stage

### Code Implementation

The OSWorld-task-collection repository is located at:  [GitHub – OSWorld-task-collection](https://github.com/yuanmengqi/OSWorld-task-collection/tree/main)

You will need to implement two main components:

1. **Task Configuration File**  stored in:  `evaluation_examples/examples`
    The configuration file should include:

    - The task ID and instruction
    - The setup steps
    - Post-processing and evaluation settings

2. **Evaluation Functions**  

   Please implement or reference the necessary functions in the following directories:`OSWorld-task-collection/desktop_env/evaluators/getters` and `OSWorld-task-collection/desktop_env/evaluators/metrics`

---

## 4. Example Task

### Example 1

#### Step 1: Write the Task Instruction

> “The file 'transcript_Hua_Li.pdf' contains the courses I have already completed, while 'Computer_Science_and_Technology_Program_Curriculum_Class_of_2025.pdf' outlines my program requirements. Please calculate the remaining credits for each module in the 'Compulsory Courses' and 'Elective Courses' sections. For the 'Core General Education Courses' module, the remaining credits should be calculated as 160 total credits minus the credits I have already earned and the remaining credits for other modules. Fill in the 'Remaining_Course.xlsx' file accordingly. Additionally, list the names of any compulsory courses I have not taken in the empty spaces under 'Remaining Courses.'”

#### Step 2: Prepare Files

- Prepare the initial files needed for setup: `Computer_Science_and_Technology_Program_Curriculum_Class_of_2025.pdf`, `transcripts_Hua_Li.pdf`, and `Remaining_Course.xlsx`.
- Prepare the ground truth file needed for evaluation: `Remaining_Course_gt.xlsx`.
- Upload all prepared files to Google Drive.

#### Step 3: Create the Task Config

Edit the task config file located at: `OSWorld-task-collection/evaluation_examples/new_tasks/example.json`

Some key parameters include:

- **"id"**: Assign a unique ID to the task and store this ID in the `new_tasks` section of the file `OSWorld-task-collection/evaluation_examples/test_new.json`.

- **"instruction"**: Enter the instruction provided in step 1.

- **"config"**: This parameter controls the setup steps. For this task, it involves downloading the initial files and opening them in LibreOffice. The code is as follows:

  ```
  "config": [
        {
          "type": "download",
          "parameters": {
            "files": [
              {
                "url": "https://drive.google.com/uc?id=1nN2RUs25QmD8EnmwOV3vqBxh7o_QQ2-j&export=download",
                "path": "/home/user/Desktop/Computer_Science_and_Technology_Program_Curriculum_Class_of_2025.pdf"
              },
              {
                "url": "https://drive.google.com/uc?id=1GWKJdtGUpng4eOxQ_HL1spHOpB--tAT_&export=download",
                "path": "/home/user/Desktop/transcripts_Hua_Li.pdf"
              },
              {
                "url": "https://drive.google.com/uc?id=16KgU8wQGgFFLEr_7IPdvY6UYecog0KoF&export=download",
                "path": "/home/user/Desktop/Remaining_Course.xlsx"
              }
            ]
          }
        },
        {
          "type": "open",
          "parameters": {
            "path": "/home/user/Desktop/Remaining_Course.xlsx"
          }
        }
      ]
  ```

- **"evaluator"**: This parameter contains the evaluation steps.

  - **"postconfig"**: This parameter includes post-processing steps. In this example, it saves the edited file. The code is as follows:

    ```
    "postconfig": [
            {
              "type": "activate_window",
              "parameters": {
                "window_name": "Remaining_Course.xlsx - LibreOffice Calc",
                "strict": true
              }
            },
            {
              "type": "sleep",
              "parameters": {
                "seconds": 0.5
              }
            },
            {
              "type": "execute",
              "parameters": {
                "command": [
                  "python",
                  "-c",
                  "import pyautogui; import time; pyautogui.hotkey('ctrl', 's'); time.sleep(0.5);"
                ]
              }
            },
            {
              "type": "sleep",
              "parameters": {
                "seconds": 3
              }
            }
          ]
    ```

  - **"func"**: This calls a function from the metrics to select the appropriate evaluation method. In this task, it uses the Excel comparison function:

    ```
    "func": "compare_table"
    ```

    - **"expected"**: This loads the ground truth file using the `getter` function. In this task, the ground truth file is downloaded:

    ```
    "expected": {
            "type": "cloud_file",
            "path": "https://drive.google.com/uc?id=1aEd-3q_78Eo5u6HPAJcd9JGP8i2NOoX9&export=download",
            "dest": "Remaining_Course_gt.xlsx"
          }
    ```

    - **"result"**: This retrieves the result of the agent's actions using the `getter` function. In this task, it loads the edited file:

    ```
    "result": {
            "type": "vm_file",
            "path": "/home/user/Desktop/Remaining_Course.xlsx",
            "dest": "Remaining_Course.xlsx"
          }
    ```

    - **"options"**: This sets parameters for the evaluation function. In this task, it specifies which cells in the Excel file to compare and how. This section depends on the specific definition of the functions in the metrics.

    ```
    "options": {
        "rules": [
            {
              "type": "sheet_fuzzy",
              "sheet_idx0": 0,
              "sheet_idx1": 0,
              "rules": [
                {
                  "range": [
                    "B3:E3"
                  ],
                  "type": "exact_match",
                  "trim_leadings": " ",
                  "trim_trailings": " "
                },
                {
                  "range": [
                    "C8:E8"
                  ],
                  "type": "exact_match",
                  "trim_leadings": " ",
                  "trim_trailings": " ",
                  "ignore_case": true
                },
                {
                  "range": [
                    "B7"
                  ],
                  "type": "exact_match",
                  "trim_leadings": " ",
                  "trim_trailings": " "
                }
              ]
            }
          ]
      }
    ```

#### **Step 4: Edit the necessary functions** 

You need to edit the functions in `OSWorld-task-collection/desktop_env/evaluators/getters` and `OSWorld-task-collection/desktop_env/evaluators/metrics` based on the steps mentioned above.

 (This directory already contains many functions that can be reused, and you may add new ones if necessary.)

- **Getters**: These functions are used to get the ground truth and results after the agent's actions. In this task, we use the `get_cloud_file` function to download a file from the cloud server, and the `get_vm_file` function to get a file from the VM.

  ```
  def get_cloud_file(env, config: Dict[str, Any]) -> Union[str, List[str]]:
      if not config.get("multi", False):
          paths: List[str] = [config["path"]]
          dests: List[str] = [config["dest"]]
      else:
          paths: List[str] = config["path"]
          dests: List[str] = config["dest"]
      cache_paths: List[str] = []
  
      gives: Set[int] = set(config.get("gives", [0]))
  
      for i, (p, d) in enumerate(zip(paths, dests)):
          _path = os.path.join(env.cache_dir, d)
          if i in gives:
              cache_paths.append(_path)
  
          if os.path.exists(_path):
              continue
  
          url = p
          response = requests.get(url, stream=True)
          response.raise_for_status()
  
          with open(_path, 'wb') as f:
              for chunk in response.iter_content(chunk_size=8192):
                  if chunk:
                      f.write(chunk)
  
      return cache_paths[0] if len(cache_paths)==1 else cache_paths
  ```

- **Metrics**: These functions are used to compare the ground truth and the agent's results. In this task, we use the `compare_table` function to compare Excel files:

  ```
  def compare_table(result: str, expected: str = None, **options) -> float:
      if result is None:
          logger.error("Result file path is None")
          return 0.0
  
      if not os.path.exists(result):
          logger.error(f"Result file not found: {result}")
          return 0.0
  
      try:
          logger.info(f"Loading result file: {result}")
          xlworkbookr: Workbook = openpyxl.load_workbook(filename=result)
          pdworkbookr = pd.ExcelFile(result)
          logger.info(
              f"Successfully loaded result file with sheets: {pdworkbookr.sheet_names}"
          )
      except Exception as e:
          logger.error(f"Failed to load result file {result}: {e}")
          return 0.0
      worksheetr_names: List[str] = pdworkbookr.sheet_names
  
      if expected is not None:
          xlworkbooke: Workbook = openpyxl.load_workbook(filename=expected)
          pdworkbooke = pd.ExcelFile(expected)
          worksheete_names: List[str] = pdworkbooke.sheet_names
      else:
          xlworkbooke: Workbook = None
          pdworkbooke = None
          worksheete_names: List[str] = None
  
      parse_idx: Callable[[Union[str, int], BOOK, BOOK], Tuple[BOOK, str]] = (
          functools.partial(
              _parse_sheet_idx,
              result_sheet_names=worksheetr_names,
              expected_sheet_names=worksheete_names,
          )
      )
  
      passes = True
      for r in options["rules"]:
          if r["type"] == "sheet_name":
              metric: bool = worksheetr_names == worksheete_names
              logger.debug(
                  "Assertion: %s.sheet_names == %s.sheet_names - %s",
                  result,
                  expected,
                  metric,
              )
  
          elif r["type"] == "sheet_data":
  ......
  ```

* If you define any **new functions**, please remember to import them in `__init__.py`.

#### Step 5: Verify the task implementation

We provide a script that allows you to manually execute the task on the AWS platform, replacing the agent’s behavior, to easily test whether the setup and evaluation functions are correct. The script is located at `OSWorld-task-collection/manual_examine.py`.

1. **Run the script**:

   ```
   python manual_examine.py \
       --headless \
       --observation_type screenshot \
       --result_dir ./results_new_tasks \         # Path to store evaluation results
       --test_all_meta_path evaluation_examples/test_new.json \  # JSON file containing the new task ID
       --region us-east-1 \                       # Set to your AWS region
       --domain new_tasks \                       # Folder containing the task
       --example_id 'example' \                       # Task ID
       --max_steps 3
   ```

2. **Open the VNC link** displayed in the terminal and log in to the visualization VM using the password `osworld-public-evaluation`.

   ![image-20251017173538562](/Users/yuanmengqi/Library/Application Support/typora-user-images/image-20251017173538562.png)

   ![image-20251017173822126](/Users/yuanmengqi/Library/Application Support/typora-user-images/image-20251017173822126.png)

3. **Manually complete the task in the VNC** environment and then press the enter key to begin the evaluation. Check the evaluation results to ensure the setup and evaluation functions are correct.